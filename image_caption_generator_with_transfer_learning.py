# -*- coding: utf-8 -*-
"""Image Caption Generator with Transfer Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1usD1aTloFFXjQD2SFFwJ6uWNLO_zFGRJ
"""

# STEP 1: Imports

print("=" * 80)
print("STEP 1: imports")
print("=" * 80)


import subprocess
import sys

packages = ['nltk', 'pillow']
for package in packages:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])


import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import pickle
import random
from tqdm import tqdm
import string
import re

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications.inception_v3 import preprocess_input
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Add
from tensorflow.keras.utils import to_categorical, plot_model


import nltk
nltk.download('punkt', quiet=True)
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu


print("\nChecking GPU availability...")
print(f"TensorFlow version: {tf.__version__}")
print(f"GPU Available: {tf.config.list_physical_devices('GPU')}")
os.system('nvidia-smi')


# STEP 2: Dataset download

print("\n" + "=" * 80)
print("STEP 2: Dataset Download")
print("=" * 80)

#trying flickr8 dataset instead
if not os.path.exists('Flickr8k_Dataset'):
    print("Downloading Flickr8k images...")
    os.system('wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip')
    os.system('unzip -q Flickr8k_Dataset.zip')
    print("Images downloaded")

if not os.path.exists('Flickr8k_text'):
    print("Downloading Flickr8k captions...")
    os.system('wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip')
    os.system('unzip -q Flickr8k_text.zip')
    print("Captions downloaded")


# STEP 3: Data preprocessiong

print("\n" + "=" * 80)
print("STEP 3: Data Preprocessing")
print("=" * 80)

def load_captions(filepath):

    captions_dict = {}
    with open(filepath, 'r') as f:
        for line in f:
            tokens = line.strip().split()
            if len(tokens) < 2:
                continue

            image_id = tokens[0].split('.')[0]
            caption = ' '.join(tokens[1:])

            if image_id not in captions_dict:
                captions_dict[image_id] = []
            captions_dict[image_id].append(caption)
    return captions_dict

def clean_captions(captions_dict):

    cleaned = {}
    for image_id, captions in captions_dict.items():
        cleaned[image_id] = []
        for caption in captions:

            caption = caption.lower()

            caption = caption.translate(str.maketrans('', '', string.punctuation))

            caption = re.sub(r'\b\w\b', '', caption)
            caption = re.sub(r'\d+', '', caption)

            caption = ' '.join(caption.split())

            caption = f'<start> {caption} <end>'
            cleaned[image_id].append(caption)
    return cleaned


print("Loading captions...")
captions_dict = load_captions('Flickr8k.token.txt')
print(f"Total images with captions: {len(captions_dict)}")

print("Cleaning captions...")
captions_dict = clean_captions(captions_dict)


print("\nSample cleaned captions:")
sample_id = list(captions_dict.keys())[0]
for i, caption in enumerate(captions_dict[sample_id][:3]):
    print(f"  {i+1}. {caption}")


# STEP 4: Feature extraction

print("\n" + "=" * 80)
print("STEP 4: Feature Extraction")
print("=" * 80)

def load_image_features_model():

    base_model = InceptionV3(weights='imagenet')

    model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)
    return model

def preprocess_image(img_path):

    img = load_img(img_path, target_size=(299, 299))
    img = img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = preprocess_input(img)
    return img

def extract_features(image_dir, image_ids, model, max_images=3500):

    features = {}
    print(f"Extracting features for {min(len(image_ids), max_images)} images...")


    image_ids_subset = list(image_ids)[:max_images]

    for img_id in tqdm(image_ids_subset):
        img_path = os.path.join(image_dir, f"{img_id}.jpg")
        if not os.path.exists(img_path):
            continue
        try:
            img = preprocess_image(img_path)
            feature = model.predict(img, verbose=0)
            features[img_id] = feature.flatten()
        except Exception as e:
            print(f"Error processing {img_id}: {e}")

    return features


print("Loading InceptionV3 model...")
feature_model = load_image_features_model()
print(f"Feature vector size: {feature_model.output_shape[1]}")


features_file = 'image_features.pkl'
if os.path.exists(features_file):
    print(f"Loading pre-extracted features from {features_file}")
    with open(features_file, 'rb') as f:
        image_features = pickle.load(f)
else:
    image_features = extract_features('Flicker8k_Dataset',
                                     captions_dict.keys(),
                                     feature_model,
                                     max_images=3500)

    with open(features_file, 'wb') as f:
        pickle.dump(image_features, f)
    print(f"Features saved to {features_file}")

print(f"Total images with features: {len(image_features)}")


# STEP 5:Tokensation

print("\n" + "=" * 80)
print("STEP 5: Tokenization")
print("=" * 80)


all_captions = []
for img_id in image_features.keys():
    if img_id in captions_dict:
        all_captions.extend(captions_dict[img_id])

print(f"Total captions: {len(all_captions)}")


tokenizer = Tokenizer(oov_token='<unk>')
tokenizer.fit_on_texts(all_captions)
vocab_size = len(tokenizer.word_index) + 1


max_length = max(len(caption.split()) for caption in all_captions)

print(f"Vocabulary size: {vocab_size}")
print(f"Maximum caption length: {max_length}")


print("\nSample tokenization:")
sample_caption = all_captions[0]
print(f"Original: {sample_caption}")
print(f"Tokenized: {tokenizer.texts_to_sequences([sample_caption])[0]}")


# STEP 6: Model create

print("\n" + "=" * 80)
print("STEP 6: Model creation")
print("=" * 80)

def create_model(vocab_size, max_length):



    image_input = Input(shape=(2048,), name='image_input')
    image_dense = Dropout(0.5)(image_input)
    image_dense = Dense(256, activation='relu', name='image_encoder')(image_dense)


    caption_input = Input(shape=(max_length,), name='caption_input')
    caption_embed = Embedding(vocab_size, 256, mask_zero=True, name='caption_embedding')(caption_input)
    caption_lstm = Dropout(0.5)(caption_embed)
    caption_lstm = LSTM(256, name='caption_decoder')(caption_lstm)


    merged = Add()([image_dense, caption_lstm])
    merged = Dense(256, activation='relu', name='merged_layer')(merged)
    output = Dense(vocab_size, activation='softmax', name='output')(merged)


    model = Model(inputs=[image_input, caption_input], outputs=output)
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model


model = create_model(vocab_size, max_length)
print(model.summary())


# STEP 7: Data generator

print("\n" + "=" * 80)
print("STEP 7: Data Generator")
print("=" * 80)

def data_generator(image_features, captions_dict, tokenizer, max_length, vocab_size, batch_size=32):

    image_ids = list(image_features.keys())

    while True:

        random.shuffle(image_ids)

        for i in range(0, len(image_ids), batch_size):
            batch_ids = image_ids[i:i+batch_size]

            X1, X2, y = [], [], []

            for img_id in batch_ids:
                if img_id not in captions_dict:
                    continue

                feature = image_features[img_id]

                for caption in captions_dict[img_id]:

                    seq = tokenizer.texts_to_sequences([caption])[0]


                    for j in range(1, len(seq)):

                        in_seq = pad_sequences([seq[:j]], maxlen=max_length, padding='post')[0]

                        out_word = to_categorical([seq[j]], num_classes=vocab_size)[0]

                        X1.append(feature)
                        X2.append(in_seq)
                        y.append(out_word)

            if len(X1) > 0:

                yield (np.array(X1), np.array(X2)), np.array(y)


total_samples = sum(len(captions_dict.get(img_id, [])) for img_id in image_features.keys())

samples_per_image = 0
for img_id in list(image_features.keys())[:10]:
    for cap in captions_dict.get(img_id, []):
        seq = tokenizer.texts_to_sequences([cap])[0]
        samples_per_image += len(seq) - 1
if len(list(image_features.keys())[:10]) * 5 > 0:
    samples_per_image = samples_per_image // (len(list(image_features.keys())[:10]) * 5)
else:
    samples_per_image = 1


steps_per_epoch = (len(image_features) * 5 * samples_per_image) // 32
steps_per_epoch = max(1, min(steps_per_epoch, 100))

print(f"Steps per epoch: {steps_per_epoch}")


output_signature = (
    (tf.TensorSpec(shape=(None, 2048), dtype=tf.float32), tf.TensorSpec(shape=(None, max_length), dtype=tf.int32)),
    tf.TensorSpec(shape=(None, vocab_size), dtype=tf.float32)
)

dataset = tf.data.Dataset.from_generator(
    lambda: data_generator(image_features, captions_dict, tokenizer, max_length, vocab_size, batch_size=32),
    output_signature=output_signature
)



# STEP 8: Model training

print("\n" + "=" * 80)
print("STEP 8: Model Training")
print("=" * 80)


epochs = 30
print(f"Training for {epochs} epochs...")

history = model.fit(
    dataset,
    steps_per_epoch=steps_per_epoch,
    epochs=epochs,
    verbose=1
)

print("Training complete!")


# STEP 9: Caption generation

print("\n" + "=" * 80)
print("STEP 9: Caption Generation")
print("=" * 80)

def generate_caption(model, tokenizer, image_feature, max_length):
    """Generate caption for an image"""

    caption = '<start>'

    for _ in range(max_length):

        seq = tokenizer.texts_to_sequences([caption])[0]
        seq = pad_sequences([seq], maxlen=max_length, padding='post')


        pred = model.predict([image_feature.reshape(1, -1), seq], verbose=0)
        pred_word_idx = np.argmax(pred)


        pred_word = None
        for word, idx in tokenizer.word_index.items():
            if idx == pred_word_idx:
                pred_word = word
                break

        if pred_word is None or pred_word in ['<end>', 'end']:
            break

        caption += ' ' + pred_word


    caption = caption.replace('<start>', '').strip()
    return caption


# STEP 10: Visualization

print("\n" + "=" * 80)
print("STEP 10: Visualization of Results")
print("=" * 80)

def display_image_with_caption(img_id, generated_caption, actual_captions):
    """Display image with generated and actual captions"""
    img_path = os.path.join('Flicker8k_Dataset', f"{img_id}.jpg")
    img = Image.open(img_path)

    plt.figure(figsize=(10, 8))
    plt.imshow(img)
    plt.axis('off')

    title = f"Generated: {generated_caption}\n"
    title += f"Actual: {actual_captions[0].replace('<start>', '').replace('<end>', '').strip()}"
    plt.title(title, fontsize=12, pad=20)
    plt.tight_layout()
    plt.show()


print("Generating captions for test images...\n")
test_image_ids = random.sample(list(image_features.keys()), min(5, len(image_features)))

generated_captions = []
reference_captions = []

for idx, img_id in enumerate(test_image_ids, 1):
    print(f"\n--- Test Image {idx} ---")


    feature = image_features[img_id]
    generated = generate_caption(model, tokenizer, feature, max_length)


    actual = captions_dict[img_id]

    print(f"Generated: {generated}")
    print(f"Actual: {actual[0].replace('<start>', '').replace('<end>', '').strip()}")


    display_image_with_caption(img_id, generated, actual)


    generated_captions.append(generated.split())
    reference_captions.append([cap.replace('<start>', '').replace('<end>', '').strip().split()
                               for cap in actual])


# STEP 11:BLEU scores

print("\n" + "=" * 80)
print("STEP 11: Evaluation using BLEU Score")
print("=" * 80)


bleu_1 = corpus_bleu(reference_captions, generated_captions, weights=(1.0, 0, 0, 0))
bleu_2 = corpus_bleu(reference_captions, generated_captions, weights=(0.5, 0.5, 0, 0))

print(f"BLEU-1 Score: {bleu_1:.4f}")
print(f"BLEU-2 Score: {bleu_2:.4f}")


print("\nIndividual BLEU scores:")
for i, (gen, ref) in enumerate(zip(generated_captions, reference_captions), 1):
    score = sentence_bleu(ref, gen, weights=(1.0, 0, 0, 0))
    print(f"Image {i}: {score:.4f}")


# STEP 13: Uploading images foer caption generation

print("\n" + "=" * 80)
print("STEP 13: Upload Your Own Images")
print("=" * 80)

from google.colab import files
from IPython.display import display, Image as IPImage

def caption_uploaded_image(model, feature_model, tokenizer, max_length):
    print("Upload your imag...")
    uploaded = files.upload()

    for filename in uploaded.keys():
        print(f"\nProcessing: {filename}")

        try:

            img = Image.open(filename)
            plt.figure(figsize=(10, 8))
            plt.imshow(img)
            plt.axis('off')
            plt.title("Your Uploaded Image", fontsize=14, pad=10)
            plt.tight_layout()
            plt.show()


            print("Extracting image features...")
            img_array = preprocess_image(filename)
            feature = feature_model.predict(img_array, verbose=0)
            feature = feature.flatten()


            print("Generating caption...")
            generated = generate_caption(model, tokenizer, feature, max_length)


            print("\n" + "="*50)
            print(f" Generated Caption: {generated}")
            print("="*50)


            plt.figure(figsize=(10, 8))
            plt.imshow(img)
            plt.axis('off')
            plt.title(f"Generated: {generated}", fontsize=14, pad=20,
                     fontweight='bold', color='darkblue')
            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f" Error processing {filename}: {e}")
            print("Please make sure the file is a valid image")


print("\nReady to caption your own images!")
print("Run the cell below to upload and caption your images:\n")


while True:
    caption_uploaded_image(model, feature_model, tokenizer, max_length)


    another = input("\nUpload another image? (yes/no): ").strip().lower()
    if another not in ['yes', 'y']:
        print("\nDone with custom image captioning!")
        break

